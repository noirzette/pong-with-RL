{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ecb700",
   "metadata": {},
   "source": [
    "# Learning to Play Pong\n",
    "\n",
    "This notebook uses mushroomrl (https://mushroomrl.readthedocs.io/en/latest/index.html) and the Pong class to run an experiment to learn to play a simplified version of Pong.\n",
    "\n",
    "The simplified Pong consists of a ball bouncing in a 2-D box (no gravity). The left, right and top sides of the box are present and reflect the ball if it hits them, but the bottom side of the box is missing. If the ball moves toward the bottom of the box, the agent has to move a short paddle along the bottom of the box to reflect the ball, otherwise the ball falls out of the box and the game is lost. When not being used to reflect the ball, the paddle should return to the bottom left.\n",
    "\n",
    "Originally I was planning to use manim to visualise the experiments, but mushroomrl has as a utilitiy that uses pygame to visualise experiment runs so I decided try that instead. The original mushroomrl code had a bug which caused the visualisation to hang, so a corrected version is included in pong.py. The window in which the visualisation runs often appears behind other windows, so you might need to look for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137afd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Import all the things\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from mushroom_rl.algorithms.value import FQI\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "\n",
    "from pong import Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6579f19a",
   "metadata": {},
   "source": [
    "We begin by assigning the Markov Decision Process (MDP), which defines how the agent will move in the environment according to a given action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f8d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = Pong()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76051376",
   "metadata": {},
   "source": [
    "Now we define the agent.\n",
    "\n",
    "- First we create the policy to be followed by the agent. Here we use a greedy algorithm, with `epsilon = 1`.\n",
    "\n",
    "- Next, the policy approximator and its required paramaters iare created; here we use the `ExtraTreesRegressor` class of scikit-learn. \n",
    "\n",
    "- Finally the agent is created calling the algorithm class and providing the approximator and the policy, together with parameters used by the algorithm. Here we use Fitted Q-iteration (FQI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c80be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "\n",
    "# Policy\n",
    "epsilon = Parameter(value=1.)\n",
    "pi = EpsGreedy(epsilon=epsilon)\n",
    "\n",
    "# Approximator\n",
    "approximator_params = dict(input_shape=mdp.info.observation_space.shape,\n",
    "                           n_actions=mdp.info.action_space.n,\n",
    "                           n_estimators=50,\n",
    "                           min_samples_split=5,\n",
    "                           min_samples_leaf=2)\n",
    "approximator = ExtraTreesRegressor\n",
    "\n",
    "# Agent\n",
    "agent = FQI(mdp.info, pi, approximator, n_iterations=20,\n",
    "            approximator_params=approximator_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db279f",
   "metadata": {},
   "source": [
    "Next we call the `core` module, which contains the function to learn in the MDP and evaluate the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24914b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core(agent, mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f2515",
   "metadata": {},
   "source": [
    "Now we're ready to learn! Here we train the agent. The agent’s policy is fitted after 400 random game samples have been collected, and this process is repeated 400 times. 400 * 400 was the smallest integration time that yielded good learning for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f22cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████▎    | 353/400 [00:00<00:00, 955.76it/s]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:03,  5.43it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:05,  3.38it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:07,  2.24it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:09,  1.71it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:10,  1.49it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:03<00:10,  1.38it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:04<00:09,  1.32it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:05<00:09,  1.29it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:06<00:08,  1.27it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:06<00:07,  1.26it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:07<00:07,  1.25it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:08<00:06,  1.25it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:09<00:05,  1.25it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:10<00:04,  1.24it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:10<00:04,  1.24it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:11<00:03,  1.25it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:12<00:02,  1.25it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:13<00:01,  1.25it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:14<00:00,  1.25it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:14<00:00,  1.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\r"
     ]
    }
   ],
   "source": [
    "core.learn(n_episodes=400, n_episodes_per_fit=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e7d7f",
   "metadata": {},
   "source": [
    "And now we evaluate the agent's learning. We compute the performance of the agent through the collected rewards during an evaluation run with randomly assigned starting position and velocity. Fixing `epsilon = 0`, the greedy policy is applied starting from the provided initial states, then the average cumulative discounted reward is displayed.\n",
    "\n",
    "If `render` is set to `True` for the dataset, then a pygame window will display the game. However, the pygame window which displays the game may well open behind your browser, so you might not see it unless you look for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc9801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average cumulative discounted reward is [2.011039317821579]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pi.set_epsilon(Parameter(0.))\n",
    "x_start = random.uniform(0, 1)\n",
    "y_start = random.uniform(0, 1)\n",
    "vx_start = random.uniform(-1, 1)\n",
    "vy_start = random.uniform(-1, 1)\n",
    "initial_state = np.array([[x_start, y_start, vx_start, vy_start]])\n",
    "dataset = core.evaluate(initial_states=initial_state, render=True)\n",
    "\n",
    "print(\"The average cumulative discounted reward is\", compute_J(dataset, gamma=mdp.info.gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd68bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a0c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
